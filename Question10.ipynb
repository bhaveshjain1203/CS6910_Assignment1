{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN0BvcdsPqUl6ohNTy4AE53",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CS22M029/cs6910_assignment1/blob/main/Question10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bDKDdVEmoMxt",
        "outputId": "88f19ba0-97e1-42e6-9224-5a2939991512"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.14.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (63.4.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.9.4)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.27.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.19.6)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.17.0-py2.py3-none-any.whl (189 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.1/189.1 KB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from wandb) (6.0)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting appdirs>=1.4.3\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=7937f55f90175d94241dd21470dd408ad5d810326cdf2df11c68b3fe8660260e\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, appdirs, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.31 appdirs-1.4.4 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.17.0 setproctitle-1.3.2 smmap-5.0.0 wandb-0.14.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230319_113330-rzllepil</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/cs22m029/cs6910_assignment1/runs/rzllepil' target=\"_blank\">leafy-pine-220</a></strong> to <a href='https://wandb.ai/cs22m029/cs6910_assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/cs22m029/cs6910_assignment1' target=\"_blank\">https://wandb.ai/cs22m029/cs6910_assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/cs22m029/cs6910_assignment1/runs/rzllepil' target=\"_blank\">https://wandb.ai/cs22m029/cs6910_assignment1/runs/rzllepil</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Nesterov Adam Adaptive Moments optimizer\n",
            "epoch:  0\n",
            "Train Accuracy :  0.9560555555555555\n",
            "Validation Accuracy :  0.955\n",
            "train loss 0.2674861228557813\n",
            "validation loss: 0.15654599073063838\n",
            "epoch:  1\n",
            "Train Accuracy :  0.967925925925926\n",
            "Validation Accuracy :  0.9631666666666666\n",
            "train loss 0.13014562709324298\n",
            "validation loss: 0.1217064988013449\n",
            "epoch:  2\n",
            "Train Accuracy :  0.9757037037037037\n",
            "Validation Accuracy :  0.9688333333333333\n",
            "train loss 0.09817429751527916\n",
            "validation loss: 0.10130460035300277\n",
            "epoch:  3\n",
            "Train Accuracy :  0.9812222222222222\n",
            "Validation Accuracy :  0.9723333333333334\n",
            "train loss 0.07869334222512009\n",
            "validation loss: 0.09003287776005879\n",
            "epoch:  4\n",
            "Train Accuracy :  0.9846111111111111\n",
            "Validation Accuracy :  0.9753333333333334\n",
            "train loss 0.06456842887932815\n",
            "validation loss: 0.08189327498259465\n",
            "epoch:  5\n",
            "Train Accuracy :  0.9880740740740741\n",
            "Validation Accuracy :  0.9766666666666667\n",
            "train loss 0.05332048438958507\n",
            "validation loss: 0.07697630651399905\n",
            "epoch:  6\n",
            "Train Accuracy :  0.9903148148148149\n",
            "Validation Accuracy :  0.9766666666666667\n",
            "train loss 0.04400821779003918\n",
            "validation loss: 0.07437360740419939\n",
            "epoch:  7\n",
            "Train Accuracy :  0.9917962962962963\n",
            "Validation Accuracy :  0.9773333333333334\n",
            "train loss 0.036320696321314024\n",
            "validation loss: 0.07323196453198667\n",
            "epoch:  8\n",
            "Train Accuracy :  0.9931296296296296\n",
            "Validation Accuracy :  0.9771666666666666\n",
            "train loss 0.029725767595905915\n",
            "validation loss: 0.07305916849289375\n",
            "epoch:  9\n",
            "Train Accuracy :  0.9942222222222222\n",
            "Validation Accuracy :  0.9775\n",
            "train loss 0.02415056480929929\n",
            "validation loss: 0.07198367794789134\n",
            "Test Accuracy 0.9764\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb\n",
        "import wandb\n",
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.datasets import mnist\n",
        "\n",
        "\n",
        "default_p = dict(\n",
        "    num_layers=3,\n",
        "    hidden_size=128,\n",
        "    learning_rate=0.001,\n",
        "    num_epochs=10,\n",
        "    batch_size=64,\n",
        "    activation=\"ReLU\",\n",
        "    optimizer=\"nadam\",\n",
        "    weight_init=\"Xavier\",\n",
        "\t  weight_decay=0,\t\n",
        "    loss_fun=\"cross_entropy\",\n",
        "    dataset=\"mnist\"\n",
        ")\n",
        "\n",
        "run = wandb.init(config=default_p, project=\"cs6910_assignment1\",\n",
        "                 entity=\"cs22m029\", reinit='True')\n",
        "config = wandb.config\n",
        "\n",
        "beta = 0.9\n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "epsilon = 1e-3\n",
        "dataset = config.dataset\n",
        "loss_function=config.loss_fun\n",
        "num_layers = config.num_layers\n",
        "hidden_size = config.hidden_size\n",
        "learning_rate = config.learning_rate\n",
        "num_epochs = config.num_epochs\n",
        "batch_size = config.batch_size\n",
        "activation = config.activation\n",
        "optimizer = config.optimizer\n",
        "weight_init = config.weight_init\n",
        "alpha=config.weight_decay\n",
        "\n",
        "# Load the dataset\n",
        "if (dataset==\"fashion_mnist\"):\n",
        "    # Load the Fashion-MNIST dataset\n",
        "    (x_Train, y_Train), (x_Test, y_Test) = fashion_mnist.load_data()\n",
        "    \n",
        "elif (dataset==\"mnist\"):\n",
        "    # Load the MNIST dataset\n",
        "    (x_Train, y_Train), (x_Test, y_Test) =mnist.load_data()\n",
        "\n",
        "# Normalize image bw 0 and 1\n",
        "x_Train = x_Train / 255\n",
        "x_Test = x_Test / 255\n",
        "\n",
        "# Split the data into training and validation sets (90:10)\n",
        "x, y, z = (x_Train.shape)\n",
        "np.random.seed(0)\n",
        "position = np.arange(x)\n",
        "np.random.shuffle(position)\n",
        "# Validation set size as 10% of total data\n",
        "val_size = int(x * 0.1)  \n",
        "\n",
        "x_Val = x_Train[position[:val_size]]\n",
        "y_Val = y_Train[position[:val_size]]\n",
        "x_Train = x_Train[position[val_size:]]\n",
        "y_Train = y_Train[position[val_size:]]\n",
        "\n",
        "# Reshape the data to (number of samples, number of features)\n",
        "x, y, z = (x_Train.shape)\n",
        "x1, y1, z1 = (x_Test.shape)\n",
        "x2, y2, z2 = (x_Val.shape)\n",
        "x_Train = np.reshape(x_Train, (x, y*z))\n",
        "x_Test = np.reshape(x_Test, (x1, y1*z1))\n",
        "x_Val = np.reshape(x_Val, (x2, y2*z2))\n",
        "\n",
        "# Define the 10 classes in the Fashion-MNIST dataset\n",
        "class_Names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "# Define the apply_Activation functions\n",
        "\n",
        "\n",
        "def apply_Activation(z, activation):\n",
        "    if activation == \"sigmoid\":\n",
        "        return 1/(1+np.exp(-z))\n",
        "    if activation == \"tanh\":\n",
        "        return np.tanh(z)\n",
        "    if activation == \"ReLU\":  # Rectified Linear Unit\n",
        "        return np.maximum(0, z)\n",
        "\n",
        "# Define the derivative of apply_Activation functions\n",
        "\n",
        "\n",
        "def act_derivative(z, activation):\n",
        "    if activation == \"sigmoid\":\n",
        "        return np.multiply(apply_Activation(z, activation), (1-apply_Activation(z, activation)))\n",
        "    if activation == \"tanh\":\n",
        "        return 1-(np.tanh(z))**2\n",
        "    if activation == \"ReLU\":\n",
        "        return np.where(z > 0, 1, 0)\n",
        "\n",
        "# defining softmax function\n",
        "\n",
        "\n",
        "def softmax(z):\n",
        "    sum_exp = np.sum(np.exp(z), axis=1, keepdims=True)\n",
        "    return np.exp(z)/sum_exp\n",
        "\n",
        "\n",
        "# Define the loss function\n",
        "def loss(y_onehot, y_hat,y,loss_function,alpha):\n",
        "# y_onehot: original values\n",
        "# y_hat: calculated probabilities\n",
        "# alpha: weight decay inclusion\n",
        "    \n",
        "    if loss_function == \"cross_entropy\":    \n",
        "        total_loss_value=-np.sum(y_onehot * np.log(y_hat)) / y.shape[0]\n",
        "        regularization_term = 0.5* alpha * np.sum(np.square(y_hat))/ y.shape[0]\n",
        "        loss_value = (total_loss_value + regularization_term) \n",
        "        return loss_value\n",
        "\n",
        "    elif loss_function == \"mean_squared_error\":\n",
        "        total_loss_value = np.sum(np.square(y_onehot - y_hat))\n",
        "        regularization_term = 0.5* alpha * np.sum(np.square(y_hat))\n",
        "        loss_value = (total_loss_value + regularization_term) / y.shape[0]\n",
        "        return loss_value\n",
        "\n",
        "\n",
        "\n",
        "# Initialize weights and biases for all the layers\n",
        "\n",
        "\n",
        "def initialize(num_layers, hidden_size, weight_init):\n",
        "    np.random.seed(0)\n",
        "    layers = []\n",
        "    layers.append(784)\n",
        "    for i in range(num_layers):\n",
        "        layers.append(hidden_size)\n",
        "    layers.append(10)\n",
        "    w = []\n",
        "    b = []\n",
        "    num_layers = len(layers)-1\n",
        "    if weight_init==\"random\":\n",
        "        for i in range(num_layers):\n",
        "            w_i = np.random.uniform(-1, 1, (layers[i+1], layers[i]))\n",
        "            b_i = np.random.uniform(-1, 1, (layers[i+1]))\n",
        "            w.append(w_i)\n",
        "            b.append(b_i)\n",
        "    elif weight_init==\"Xavier\":\n",
        "        for i in range(num_layers):\n",
        "            x=np.sqrt(6/(layers[i+1]+layers[i]))\n",
        "            w_i = np.random.uniform(-x, x, (layers[i+1], layers[i]))\n",
        "            b_i = np.random.uniform(-x, x, (layers[i+1]))\n",
        "            w.append(w_i)\n",
        "            b.append(b_i)\n",
        "    return w, b, layers\n",
        "\n",
        "# checking the accuracy of data\n",
        "\n",
        "\n",
        "def accuracy(x, y, w, b, activation):\n",
        "    a, h = forward_propagation(w, b, x, activation)\n",
        "    y_hat_hot = h[-1]\n",
        "    y_hat = np.argmax(y_hat_hot, axis=1)\n",
        "    accuracy = 1 - np.mean(y_hat != y)\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def onehot(y):\n",
        "    # make one hot vector out of y\n",
        "    num_op = 10\n",
        "    y_onehot = np.zeros((y.shape[0], num_op))\n",
        "    for i in range(y.shape[0]):\n",
        "        y_onehot[i][y[i]] = 1\n",
        "    return y_onehot\n",
        "\n",
        "# Define the forward propagation function\n",
        "\n",
        "\n",
        "def forward_propagation(w, b, x, activation):\n",
        "    a = []  # preactivation\n",
        "    h = []  # apply_Activation\n",
        "    num_layers = len(w)\n",
        "\n",
        "    for i in range(num_layers-1):\n",
        "        w[i] = np.array(w[i])\n",
        "        b[i] = np.array(b[i])\n",
        "        a_i = np.dot(x, w[i].T) + b[i]\n",
        "        h_i = apply_Activation(a_i, activation)\n",
        "        a.append(a_i)\n",
        "        h.append(h_i)\n",
        "        x = h_i\n",
        "\n",
        "    # for last layer use softmax function\n",
        "    w[-1] = np.array(w[-1])\n",
        "    b[-1] = np.array(b[-1])\n",
        "    a_last = np.matmul(x, w[-1].T) + b[-1]\n",
        "    h_last = softmax(a_last)\n",
        "    a.append(a_last)\n",
        "    h.append(h_last)\n",
        "    return a, h\n",
        "\n",
        "\n",
        "# Define the backward propagation function\n",
        "def backward_propagation(a, h, y_Train, y_hat, y_onehot, x_Train, w, b, activation, alpha):\n",
        "\n",
        "    dw = []\n",
        "    db = []\n",
        "    dh = []\n",
        "    da = []\n",
        "\n",
        "    # compute o/p gradients\n",
        "    da_last = -(y_onehot-y_hat)\n",
        "    dh_last = -(y_onehot/y_hat)\n",
        "    da.append(da_last)\n",
        "    dh.append(dh_last)\n",
        "\n",
        "    n = len(w)-1\n",
        "    for i in range(n, 0, -1):\n",
        "        # compute gradients wrt params:\n",
        "        dw_i = np.dot(da[-1].T, h[i-1])/y_Train.shape[0]\n",
        "\n",
        "        dw.append(dw_i)\n",
        "        db_i = np.sum(da[-1], axis=0)/y_Train.shape[0]\n",
        "        db.append(db_i)\n",
        "\n",
        "        # compute gradients wrt layer below:\n",
        "        dh_i = np.dot(da[-1], w[i])\n",
        "        dh.append(dh_i)\n",
        "\n",
        "        # compute gradients wrt layer below (pre-apply_Activation) :\n",
        "        da_i = np.multiply(dh[-1], act_derivative(a[i-1], activation))\n",
        "        da.append(da_i)\n",
        "\n",
        "    # computing w0 and b0\n",
        "    dw_i = np.dot(da[-1].T, x_Train)/y_Train.shape[0]\n",
        "    dw.append(dw_i)\n",
        "    db_i = np.sum(da[-1], axis=0)/y_Train.shape[0]\n",
        "    db.append(db_i)\n",
        "\n",
        "    dw.reverse()\n",
        "    db.reverse()\n",
        "    #for L2 Regularization\n",
        "    for i in range(len(dw)):\n",
        "        dw[i]=np.add(dw[i],alpha*w[i])\n",
        "    return dw, db\n",
        "\n",
        "\n",
        "# training to find best weights and biases (with batches)\n",
        "def gradient_descent_with_batch_size(x_Train, y_Train, layers, w, b, learning_rate, activation, num_epochs, batch_size, loss_function, alpha):\n",
        "\n",
        "    print(\"Gradient Descent with Batch Size = \", batch_size)\n",
        "\n",
        "    # make one hot vector out of y_Train and y_Val\n",
        "    y_onehot = onehot(y_Train)\n",
        "    y_onehot_val = onehot(y_Val)\n",
        "\n",
        "    losses = []\n",
        "\n",
        "    # dividing data in batches\n",
        "    num_samples = x_Train.shape[0]\n",
        "    num_batches = num_samples // batch_size\n",
        "\n",
        "    # Loop through the specified number of epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        train_epoch_loss = 0.0\n",
        "        val_epoch_loss = 0.0\n",
        "\n",
        "        # Loop through each batch in the training data\n",
        "        for batch_idx in range(num_batches):\n",
        "            start = batch_idx * batch_size\n",
        "            end = start + batch_size\n",
        "\n",
        "            x_Train_batch = x_Train[start:end, :]\n",
        "            y_Train_batch = y_Train[start:end]\n",
        "            y_onehot_batch = y_onehot[start:end, :]\n",
        "\n",
        "            # Forward Propagation on training data batch\n",
        "            a, h = forward_propagation(w, b, x_Train_batch, activation)\n",
        "\n",
        "            y_hat = h[-1]\n",
        "            train_loss_i = loss(y_onehot_batch, y_hat,y_Train,loss_function,alpha)\n",
        "            train_epoch_loss += train_loss_i\n",
        "\n",
        "            # Backward Propagation\n",
        "            dw, db = backward_propagation(\n",
        "                a, h, y_Train_batch, y_hat, y_onehot_batch, x_Train_batch, w, b, activation, alpha)\n",
        "\n",
        "            # Update the weights and biases\n",
        "            for i in range(len(w)):\n",
        "                w[i] = w[i] - learning_rate * dw[i]\n",
        "                b[i] = b[i] - learning_rate * db[i]\n",
        "\n",
        "        # Forward Propagation on val batch\n",
        "        a, h = forward_propagation(w, b, x_Val, activation)\n",
        "\n",
        "        y_hatval = h[-1]\n",
        "        val_loss_i = loss(y_onehot_val, y_hatval,y_Val, loss_function,alpha)\n",
        "        val_epoch_loss = val_loss_i\n",
        "        acc_train = accuracy(x_Train, y_Train, w, b, activation)\n",
        "        acc_val = accuracy(x_Val, y_Val, w, b, activation)\n",
        "        print(\"epoch: \", epoch)\n",
        "        print(\"Train Accuracy : \", acc_train)\n",
        "        print(\"Validation Accuracy : \", acc_val)\n",
        "     \n",
        "        print(\"train loss\", train_epoch_loss)\n",
        "        print(\"validation loss:\", val_epoch_loss)\n",
        "\n",
        "        wandb.log({'train_accuracy': acc_train,\n",
        "                   'val_accuracy': acc_val,\n",
        "                   'train_loss': train_epoch_loss,\n",
        "                   'val_loss': val_epoch_loss})\n",
        "\n",
        "    return w, b, losses\n",
        "\n",
        " # training to find best weights and biases (with momentum)\n",
        "\n",
        "\n",
        "def momentum_gradient_descent(x_Train, y_Train, layers, w, b, learning_rate, activation, num_epochs, batch_size, beta, loss_function,alpha):\n",
        "    print(\"Momentum Gradient Descent with Batch Size = \", batch_size)\n",
        "\n",
        "    # make one hot vector out of y_Train and y_Val\n",
        "    y_onehot = onehot(y_Train)\n",
        "    y_onehot_val = onehot(y_Val)\n",
        "\n",
        "    losses = []\n",
        "    # dividing data in batches\n",
        "    num_samples = x_Train.shape[0]\n",
        "    num_batches = num_samples // batch_size\n",
        "\n",
        "    prev_w = []\n",
        "    prev_b = []\n",
        "    num_layers = len(layers)-1\n",
        "    for i in range(num_layers):\n",
        "        prev_w_i = np.zeros((layers[i+1], layers[i]))\n",
        "        prev_b_i = np.zeros(layers[i+1])\n",
        "        prev_w.append(prev_w_i)\n",
        "        prev_b.append(prev_b_i)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_epoch_loss = 0.0\n",
        "        val_epoch_loss = 0.0\n",
        "\n",
        "        # Loop through each batch in the training data\n",
        "        for batch_idx in range(num_batches):\n",
        "            start = batch_idx * batch_size\n",
        "            end = start + batch_size\n",
        "\n",
        "            x_Train_batch = x_Train[start:end, :]\n",
        "            y_Train_batch = y_Train[start:end]\n",
        "            y_onehot_batch = y_onehot[start:end, :]\n",
        "\n",
        "            # Forward Propagation on training data batch\n",
        "            a, h = forward_propagation(w, b, x_Train_batch, activation)\n",
        "\n",
        "            y_hat = h[-1]\n",
        "            train_loss_i = loss(y_onehot_batch, y_hat,y_Train,loss_function,alpha)\n",
        "            train_epoch_loss += train_loss_i\n",
        "\n",
        "            # Backward Propagation\n",
        "            dw, db = backward_propagation(\n",
        "                a, h, y_Train_batch, y_hat, y_onehot_batch, x_Train_batch, w, b, activation, alpha)\n",
        "\n",
        "            # update weight and biases giving importance to history as well\n",
        "            for i in range(len(w)):\n",
        "\n",
        "                prev_w[i] = beta * prev_w[i] + learning_rate * dw[i]\n",
        "                prev_b[i] = beta * prev_b[i] + learning_rate * db[i]\n",
        "\n",
        "                w[i] = w[i] - prev_w[i]\n",
        "                b[i] = b[i] - prev_b[i]\n",
        "\n",
        "    # Forward Propagation on val batch\n",
        "        a, h = forward_propagation(w, b, x_Val, activation)\n",
        "\n",
        "        y_hatval = h[-1]\n",
        "        val_loss_i = loss(y_onehot_val, y_hatval,y_Val,loss_function,alpha)\n",
        "        val_epoch_loss = val_loss_i\n",
        "        acc_train = accuracy(x_Train, y_Train, w, b, activation)\n",
        "        acc_val = accuracy(x_Val, y_Val, w, b, activation)\n",
        "        print(\"epoch: \", epoch)\n",
        "        print(\"Train Accuracy : \", acc_train)\n",
        "        print(\"Validation Accuracy : \", acc_val)\n",
        "     \n",
        "        print(\"train loss\", train_epoch_loss)\n",
        "        print(\"validation loss:\", val_epoch_loss)\n",
        "\n",
        "        wandb.log({'train_accuracy': acc_train,\n",
        "                   'val_accuracy': acc_val,\n",
        "                   'train_loss': train_epoch_loss,\n",
        "                   'val_loss': val_epoch_loss})\n",
        "\n",
        "    return w, b, losses\n",
        "\n",
        "\n",
        "# Training to find best weights and biases (with Nesterov accelerated gradient descent)\n",
        "def nesterov_gradient_descent(x_Train, y_Train, w, b, layers, learning_rate, num_epochs, batch_size, beta, activation, loss_function,alpha):\n",
        "    print(\"Nesterov accelerated gradient descent\")\n",
        "\n",
        "    # make one hot vector out of y_Train and y_Val\n",
        "    y_onehot = onehot(y_Train)\n",
        "    y_onehot_val = onehot(y_Val)\n",
        "\n",
        "    losses = []\n",
        "    # Divide data into batches\n",
        "    num_samples = x_Train.shape[0]\n",
        "    num_batches = num_samples // batch_size\n",
        "\n",
        "    prev_w = []\n",
        "    prev_b = []\n",
        "    num_layers = len(layers)-1\n",
        "    for i in range(num_layers):\n",
        "        prev_w_i = np.zeros((layers[i+1], layers[i]))\n",
        "        prev_b_i = np.zeros(layers[i+1])\n",
        "        prev_w.append(prev_w_i)\n",
        "        prev_b.append(prev_b_i)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_epoch_loss = 0.0\n",
        "        val_epoch_loss = 0.0\n",
        "\n",
        "        # Loop through each batch in the training data\n",
        "        for batch_idx in range(num_batches):\n",
        "            start = batch_idx * batch_size\n",
        "            end = start + batch_size\n",
        "\n",
        "            x_Train_batch = x_Train[start:end, :]\n",
        "            y_Train_batch = y_Train[start:end]\n",
        "            y_onehot_batch = y_onehot[start:end, :]\n",
        "\n",
        "            # Calculate the gradients using Nesterov accelerated gradient descent\n",
        "            w_nesterov = []\n",
        "            b_nesterov = []\n",
        "            for i in range(len(w)):\n",
        "                w_nesterov_i = w[i] - beta * prev_w[i]\n",
        "                b_nesterov_i = b[i] - beta * prev_b[i]\n",
        "                w_nesterov.append(w_nesterov_i)\n",
        "                b_nesterov.append(b_nesterov_i)\n",
        "\n",
        "            # Forward propagation on training data batch\n",
        "            a, h = forward_propagation(\n",
        "                w_nesterov, b_nesterov, x_Train_batch, activation)\n",
        "\n",
        "            y_hat = h[-1]\n",
        "            train_loss_i = loss(y_onehot_batch, y_hat,y_Train,loss_function,alpha)\n",
        "            train_epoch_loss += train_loss_i\n",
        "\n",
        "            # Backward propagation\n",
        "            dw, db = backward_propagation(\n",
        "                a, h, y_Train_batch, y_hat, y_onehot_batch, x_Train_batch, w_nesterov, b_nesterov, activation, alpha)\n",
        "\n",
        "            # Update the weights and biases with Nesterov accelerated gradient descent\n",
        "            for i in range(len(w)):\n",
        "                prev_w[i] = beta * prev_w[i] + learning_rate * dw[i]\n",
        "                prev_b[i] = beta * prev_b[i] + learning_rate * db[i]\n",
        "                w[i] = w[i] - prev_w[i]\n",
        "                b[i] = b[i] - prev_b[i]\n",
        "\n",
        "        # Forward Propagation on val batch\n",
        "        a, h = forward_propagation(w, b, x_Val, activation)\n",
        "\n",
        "        y_hatval = h[-1]\n",
        "        val_loss_i = loss(y_onehot_val, y_hatval,y_Val,loss_function,alpha)\n",
        "        val_epoch_loss = val_loss_i\n",
        "        acc_train = accuracy(x_Train, y_Train, w, b, activation)\n",
        "        acc_val = accuracy(x_Val, y_Val, w, b, activation)\n",
        "        print(\"epoch: \", epoch)\n",
        "        print(\"Train Accuracy : \", acc_train)\n",
        "        print(\"Validation Accuracy : \", acc_val)\n",
        "     \n",
        "        print(\"train loss\", train_epoch_loss)\n",
        "        print(\"validation loss:\", val_epoch_loss)\n",
        "\n",
        "        wandb.log({'train_accuracy': acc_train,\n",
        "                   'val_accuracy': acc_val,\n",
        "                   'train_loss': train_epoch_loss,\n",
        "                   'val_loss': val_epoch_loss})\n",
        "\n",
        "    return w, b, losses\n",
        "\n",
        "\n",
        "# Training to find best weights and biases (with RMSProp)\n",
        "def rmsProp(x_Train, y_Train, w, b, layers, learning_rate, num_epochs, batch_size, beta, epsilon, activation, loss_function,alpha):\n",
        "    print(\"RMSProp algorithm\")\n",
        "\n",
        "    # make one hot vector out of y_Train and y_Val\n",
        "    y_onehot = onehot(y_Train)\n",
        "    y_onehot_val = onehot(y_Val)\n",
        "\n",
        "    losses = []\n",
        "    # Divide data into batches\n",
        "    num_samples = x_Train.shape[0]\n",
        "    num_batches = num_samples // batch_size\n",
        "\n",
        "    prev_w = []\n",
        "    prev_b = []\n",
        "    num_layers = len(layers)-1\n",
        "    for i in range(num_layers):\n",
        "        prev_w_i = np.zeros((layers[i+1], layers[i]))\n",
        "        prev_b_i = np.zeros(layers[i+1])\n",
        "        prev_w.append(prev_w_i)\n",
        "        prev_b.append(prev_b_i)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_epoch_loss = 0.0\n",
        "        val_epoch_loss = 0.0\n",
        "\n",
        "        # Loop through each batch in the training data\n",
        "        for batch_idx in range(num_batches):\n",
        "            start = batch_idx * batch_size\n",
        "            end = start + batch_size\n",
        "\n",
        "            x_Train_batch = x_Train[start:end, :]\n",
        "            y_Train_batch = y_Train[start:end]\n",
        "            y_onehot_batch = y_onehot[start:end, :]\n",
        "\n",
        "            # Forward propagation on training data batch\n",
        "            a, h = forward_propagation(w, b, x_Train_batch, activation)\n",
        "\n",
        "            y_hat = h[-1]\n",
        "            train_loss_i = loss(y_onehot_batch, y_hat,y_Train,loss_function,alpha)\n",
        "            train_epoch_loss += train_loss_i\n",
        "\n",
        "            # Backward propagation\n",
        "            dw, db = backward_propagation(\n",
        "                a, h, y_Train_batch, y_hat, y_onehot_batch, x_Train_batch, w, b, activation, alpha)\n",
        "\n",
        "            # Update the weights and biases with RMSProp\n",
        "            for i in range(len(w)):\n",
        "                prev_w[i] = beta * prev_w[i] + (1 - beta) * (dw[i] ** 2)\n",
        "                prev_b[i] = beta * prev_b[i] + (1 - beta) * (db[i] ** 2)\n",
        "                w[i] = w[i] - (learning_rate /\n",
        "                               (np.sqrt(prev_w[i]) + epsilon)) * dw[i]\n",
        "                b[i] = b[i] - (learning_rate /\n",
        "                               (np.sqrt(prev_b[i]) + epsilon)) * db[i]\n",
        "\n",
        "        # Forward Propagation on val batch\n",
        "        a, h = forward_propagation(w, b, x_Val, activation)\n",
        "\n",
        "        y_hatval = h[-1]\n",
        "        val_loss_i = loss(y_onehot_val, y_hatval,y_Val,loss_function,alpha)\n",
        "        val_epoch_loss = val_loss_i\n",
        "        acc_train = accuracy(x_Train, y_Train, w, b, activation)\n",
        "        acc_val = accuracy(x_Val, y_Val, w, b, activation)\n",
        "        print(\"epoch: \", epoch)\n",
        "        print(\"Train Accuracy : \", acc_train)\n",
        "        print(\"Validation Accuracy : \", acc_val)\n",
        "     \n",
        "        print(\"train loss\", train_epoch_loss)\n",
        "        print(\"validation loss:\", val_epoch_loss)\n",
        "\n",
        "        wandb.log({'train_accuracy': acc_train,\n",
        "                   'val_accuracy': acc_val,\n",
        "                   'train_loss': train_epoch_loss,\n",
        "                   'val_loss': val_epoch_loss})\n",
        "\n",
        "    return w, b, losses\n",
        "\n",
        "\n",
        "# Training to find best weights and biases (with Adaptive Moments)\n",
        "def adam(x_Train, y_Train, w, b, layers, learning_rate, num_epochs, batch_size, beta1, beta2, epsilon, activation, loss_function,alpha):\n",
        "    print(\"Adam : Adaptive Moments algorithm\")\n",
        "    # make one hot vector out of y_Train and y_Val\n",
        "    y_onehot = onehot(y_Train)\n",
        "    y_onehot_val = onehot(y_Val)\n",
        "\n",
        "    losses = []\n",
        "    # Divide data into batches\n",
        "    num_samples = x_Train.shape[0]\n",
        "    num_batches = num_samples // batch_size\n",
        "\n",
        "    m_w = []\n",
        "    m_b = []\n",
        "    prev_w = []\n",
        "    prev_b = []\n",
        "\n",
        "    num_layers = len(layers)-1\n",
        "    for i in range(num_layers):\n",
        "        m_w_i = np.zeros((layers[i+1], layers[i]))\n",
        "        m_b_i = np.zeros(layers[i+1])\n",
        "        prev_w_i = np.zeros((layers[i+1], layers[i]))\n",
        "        prev_b_i = np.zeros(layers[i+1])\n",
        "        m_w.append(m_w_i)\n",
        "        m_b.append(m_b_i)\n",
        "        prev_w.append(prev_w_i)\n",
        "        prev_b.append(prev_b_i)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_epoch_loss = 0.0\n",
        "        val_epoch_loss = 0.0\n",
        "\n",
        "        # Loop through each batch in the training data\n",
        "        for batch_idx in range(num_batches):\n",
        "            start = batch_idx * batch_size\n",
        "            end = start + batch_size\n",
        "\n",
        "            x_Train_batch = x_Train[start:end, :]\n",
        "            y_Train_batch = y_Train[start:end]\n",
        "            y_onehot_batch = y_onehot[start:end, :]\n",
        "\n",
        "            # Forward propagation on training data batch\n",
        "            a, h = forward_propagation(w, b, x_Train_batch, activation)\n",
        "\n",
        "            y_hat = h[-1]\n",
        "            train_loss_i = loss(y_onehot_batch, y_hat,y_Train,loss_function,alpha)\n",
        "            train_epoch_loss += train_loss_i\n",
        "\n",
        "            # compute the gradients by Backward propagation\n",
        "            dw, db = backward_propagation(\n",
        "                a, h, y_Train_batch, y_hat, y_onehot_batch, x_Train_batch, w, b, activation, alpha)\n",
        "\n",
        "            # Update the weights and biases with adam\n",
        "            for i in range(len(w)):\n",
        "                m_w[i] = beta1 * m_w[i] + (1 - beta1) * dw[i]\n",
        "                m_b[i] = beta1 * m_b[i] + (1 - beta1) * db[i]\n",
        "                prev_w[i] = beta2 * prev_w[i] + (1 - beta2) * (dw[i] ** 2)\n",
        "                prev_b[i] = beta2 * prev_b[i] + (1 - beta2) * (db[i] ** 2)\n",
        "\n",
        "                m_w_hat = m_w[i]/(1-np.power(beta1, i+1))\n",
        "                m_b_hat = m_b[i]/(1-np.power(beta1, i+1))\n",
        "                prev_w_hat = prev_w[i]/(1-np.power(beta2, i+1))\n",
        "                prev_b_hat = prev_b[i]/(1-np.power(beta2, i+1))\n",
        "\n",
        "                # update parameters\n",
        "                w[i] = w[i] - (learning_rate * m_w_hat /\n",
        "                               (np.sqrt(prev_w_hat)+epsilon))\n",
        "                b[i] = b[i] - (learning_rate * m_b_hat /\n",
        "                               (np.sqrt(prev_b_hat)+epsilon))\n",
        "\n",
        "        # Forward Propagation on val batch\n",
        "        a, h = forward_propagation(w, b, x_Val, activation)\n",
        "\n",
        "        y_hatval = h[-1]\n",
        "        val_loss_i = loss(y_onehot_val, y_hatval,y_Val,loss_function,alpha)\n",
        "        val_epoch_loss = val_loss_i\n",
        "        acc_train = accuracy(x_Train, y_Train, w, b, activation)\n",
        "        acc_val = accuracy(x_Val, y_Val, w, b, activation)\n",
        "        print(\"epoch: \", epoch)\n",
        "        print(\"Train Accuracy : \", acc_train)\n",
        "        print(\"Validation Accuracy : \", acc_val)\n",
        "     \n",
        "        print(\"train loss\", train_epoch_loss)\n",
        "        print(\"validation loss:\", val_epoch_loss)\n",
        "\n",
        "        wandb.log({'train_accuracy': acc_train,\n",
        "                   'val_accuracy': acc_val,\n",
        "                   'train_loss': train_epoch_loss,\n",
        "                   'val_loss': val_epoch_loss})\n",
        "\n",
        "    return w, b, losses\n",
        "\n",
        "# Training to find best weights and biases (with Nesterov Adaptive Moments)\n",
        "def nadam(x_Train, y_Train, w, b, layers, learning_rate, num_epochs, batch_size, beta1, beta2, epsilon, activation, loss_function,alpha):\n",
        "\n",
        "    print(\"Nesterov Adam Adaptive Moments optimizer\")\n",
        "    # x_Train, y_Train, layers, Learning rate, max epochs, batch size, beta1, beta2, epsilon)\n",
        "\n",
        "    # make one hot vector out of y_Train and y_Val\n",
        "    y_onehot = onehot(y_Train)\n",
        "    y_onehot_val = onehot(y_Val)\n",
        "\n",
        "    losses = []\n",
        "    # Divide data into batches\n",
        "    num_samples = x_Train.shape[0]\n",
        "    num_batches = num_samples // batch_size\n",
        "\n",
        "    m_w = []\n",
        "    m_b = []\n",
        "    prev_w = []\n",
        "    prev_b = []\n",
        "\n",
        "    num_layers = len(layers)-1\n",
        "    for i in range(num_layers):\n",
        "        m_w_i = np.zeros((layers[i+1], layers[i]))\n",
        "        m_b_i = np.zeros(layers[i+1])\n",
        "        prev_w_i = np.zeros((layers[i+1], layers[i]))\n",
        "        prev_b_i = np.zeros(layers[i+1])\n",
        "        m_w.append(m_w_i)\n",
        "        m_b.append(m_b_i)\n",
        "        prev_w.append(prev_w_i)\n",
        "        prev_b.append(prev_b_i)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_epoch_loss = 0.0\n",
        "        val_epoch_loss = 0.0\n",
        "\n",
        "        # Loop through each batch in the training data\n",
        "        for batch_idx in range(num_batches):\n",
        "            start = batch_idx * batch_size\n",
        "            end = start + batch_size\n",
        "\n",
        "            x_Train_batch = x_Train[start:end, :]\n",
        "            y_Train_batch = y_Train[start:end]\n",
        "            y_onehot_batch = y_onehot[start:end, :]\n",
        "\n",
        "            # Forward propagation on training data batch\n",
        "            a, h = forward_propagation(w, b, x_Train_batch, activation)\n",
        "\n",
        "            y_hat = h[-1]\n",
        "            train_loss_i = loss(y_onehot_batch, y_hat,y_Train,loss_function,alpha)\n",
        "            train_epoch_loss += train_loss_i\n",
        "\n",
        "            # compute the gradients by Backward propagation\n",
        "            dw, db = backward_propagation(\n",
        "                a, h, y_Train_batch, y_hat, y_onehot_batch, x_Train_batch, w, b, activation, alpha)\n",
        "\n",
        "            # Update the weights and biases with nadam\n",
        "            for i in range(len(w)):\n",
        "                m_w[i] = beta1 * m_w[i] + (1 - beta1) * dw[i]\n",
        "                m_b[i] = beta1 * m_b[i] + (1 - beta1) * db[i]\n",
        "                prev_w[i] = beta2 * prev_w[i] + (1 - beta2) * (dw[i] ** 2)\n",
        "                prev_b[i] = beta2 * prev_b[i] + (1 - beta2) * (db[i] ** 2)\n",
        "\n",
        "                m_w_hat = m_w[i]/(1-np.power(beta1, i+1))\n",
        "                m_b_hat = m_b[i]/(1-np.power(beta1, i+1))\n",
        "                prev_w_hat = prev_w[i]/(1-np.power(beta2, i+1))\n",
        "                prev_b_hat = prev_b[i]/(1-np.power(beta2, i+1))\n",
        "\n",
        "                m_w_dash = beta1 * m_w_hat + \\\n",
        "                    (1-beta1) * dw[i] / (1-np.power(beta1, i+1))\n",
        "                m_b_dash = beta1 * m_b_hat + \\\n",
        "                    (1-beta1) * db[i] / (1-np.power(beta1, i+1))\n",
        "\n",
        "                # update parameters\n",
        "                w[i] = w[i] - (learning_rate * m_w_dash /\n",
        "                               (np.sqrt(prev_w_hat)+epsilon))\n",
        "                b[i] = b[i] - (learning_rate * m_b_dash /\n",
        "                               (np.sqrt(prev_b_hat)+epsilon))\n",
        "\n",
        "        # Forward Propagation on val batch\n",
        "        a, h = forward_propagation(w, b, x_Val, activation)\n",
        "\n",
        "        y_hatval = h[-1]\n",
        "        val_loss_i = loss(y_onehot_val, y_hatval,y_Val,loss_function,alpha)\n",
        "        val_epoch_loss = val_loss_i\n",
        "        acc_train = accuracy(x_Train, y_Train, w, b, activation)\n",
        "        acc_val = accuracy(x_Val, y_Val, w, b, activation)\n",
        "        print(\"epoch: \", epoch)\n",
        "        print(\"Train Accuracy : \", acc_train)\n",
        "        print(\"Validation Accuracy : \", acc_val)\n",
        "     \n",
        "        print(\"train loss\", train_epoch_loss)\n",
        "        print(\"validation loss:\", val_epoch_loss)\n",
        "\n",
        "        wandb.log({'train_accuracy': acc_train,\n",
        "                   'val_accuracy': acc_val,\n",
        "                   'train_loss': train_epoch_loss,\n",
        "                   'val_loss': val_epoch_loss})\n",
        "\n",
        "    \n",
        "\n",
        "    return w, b, losses\n",
        "\n",
        "\n",
        "def Neural_Network(num_layers, hidden_size, learning_rate, num_epochs, batch_size, beta,\n",
        "                   beta1, beta2, epsilon, activation, optimizer, loss_function, weight_init,alpha):\n",
        "\n",
        "    # Initialize the weights w0 and biases b0 and layers of neural network\n",
        "    w, b, layers = initialize(num_layers, hidden_size,weight_init)\n",
        "\n",
        "    if optimizer == 'sgd':\n",
        "        w, b, loss_history = gradient_descent_with_batch_size(x_Train, y_Train, layers, w, b, learning_rate, activation,\n",
        "                                                              num_epochs, batch_size, loss_function,alpha)\n",
        "    elif optimizer == 'momentum':\n",
        "        w, b, loss_history = momentum_gradient_descent(x_Train, y_Train, layers, w, b, learning_rate, activation,\n",
        "                                                       num_epochs, batch_size, beta, loss_function,alpha)\n",
        "    elif optimizer == 'nesterov':\n",
        "        w, b, loss_history = nesterov_gradient_descent(x_Train, y_Train, w, b, layers,   learning_rate,\n",
        "                                                       num_epochs, batch_size, beta, activation, loss_function,alpha)\n",
        "    elif optimizer == 'rmsprop':\n",
        "        w, b, loss_history = rmsProp(x_Train, y_Train, w, b, layers,  learning_rate,\n",
        "                                     num_epochs, batch_size, beta, epsilon,activation, loss_function,alpha)\n",
        "    elif optimizer == 'adam':\n",
        "        w, b, loss_history = adam(x_Train, y_Train, w, b, layers,   learning_rate,\n",
        "                                  num_epochs, batch_size, beta1, beta2, epsilon,activation, loss_function,alpha)\n",
        "    elif optimizer == 'nadam':\n",
        "        w, b, loss_history = nadam(x_Train, y_Train, w, b, layers,  learning_rate,\n",
        "                                   num_epochs, batch_size, beta1, beta2, epsilon,activation, loss_function,alpha)\n",
        "    \n",
        "    a, h = forward_propagation(w, b, x_Test, activation)\n",
        "    y_hat_hot = h[-1]\n",
        "    y_hat = np.argmax(y_hat_hot, axis=1)\n",
        "    accuracy = 1 - np.mean(y_hat != y_Test)\n",
        "    print(\"Test Accuracy\",accuracy)\n",
        "\n",
        "\n",
        "    wandb.log({\"conf_mat\" : wandb.plot.confusion_matrix(probs=None,\n",
        "                        y_true=y_Test, preds=y_hat,\n",
        "                        class_names=class_Names)})\n",
        "\n",
        "\n",
        "run.name = \"hl_\"+str(num_layers)+\"_bs_\"+str(batch_size) + \\\n",
        "    \"_ac_\"+activation+\"_op_\"+optimizer\n",
        "\n",
        "\n",
        "Neural_Network ( num_layers, hidden_size, learning_rate, num_epochs, batch_size, beta, beta1,\n",
        "               beta2, epsilon, activation, optimizer,loss_function, weight_init,alpha ) \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_layers=4\n",
        "Neural_Network ( num_layers, hidden_size, learning_rate, num_epochs, batch_size, beta, beta1,\n",
        "               beta2, epsilon, activation, optimizer,loss_function, weight_init,alpha ) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hE4dREGestTq",
        "outputId": "8f67f01d-a255-4c79-b0f6-b35b925fd6cc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nesterov Adam Adaptive Moments optimizer\n",
            "epoch:  0\n",
            "Train Accuracy :  0.9616666666666667\n",
            "Validation Accuracy :  0.959\n",
            "train loss 0.25337081234230735\n",
            "validation loss: 0.14071009067485057\n",
            "epoch:  1\n",
            "Train Accuracy :  0.9720185185185185\n",
            "Validation Accuracy :  0.9646666666666667\n",
            "train loss 0.11892382017560102\n",
            "validation loss: 0.11014389043788496\n",
            "epoch:  2\n",
            "Train Accuracy :  0.9771481481481481\n",
            "Validation Accuracy :  0.9686666666666667\n",
            "train loss 0.0901826056961489\n",
            "validation loss: 0.09798252103765233\n",
            "epoch:  3\n",
            "Train Accuracy :  0.9812222222222222\n",
            "Validation Accuracy :  0.9718333333333333\n",
            "train loss 0.0721694331558552\n",
            "validation loss: 0.08881410207689258\n",
            "epoch:  4\n",
            "Train Accuracy :  0.9851666666666666\n",
            "Validation Accuracy :  0.9741666666666666\n",
            "train loss 0.0586106073516979\n",
            "validation loss: 0.08240811006786151\n",
            "epoch:  5\n",
            "Train Accuracy :  0.9889259259259259\n",
            "Validation Accuracy :  0.9761666666666666\n",
            "train loss 0.04776376449010586\n",
            "validation loss: 0.07555628078425207\n",
            "epoch:  6\n",
            "Train Accuracy :  0.9909074074074075\n",
            "Validation Accuracy :  0.9763333333333334\n",
            "train loss 0.03866082140354896\n",
            "validation loss: 0.0734724231584016\n",
            "epoch:  7\n",
            "Train Accuracy :  0.9919444444444444\n",
            "Validation Accuracy :  0.9766666666666667\n",
            "train loss 0.031126402802054476\n",
            "validation loss: 0.07403539102552242\n",
            "epoch:  8\n",
            "Train Accuracy :  0.9930925925925926\n",
            "Validation Accuracy :  0.9768333333333333\n",
            "train loss 0.02490447688154452\n",
            "validation loss: 0.07584901878113443\n",
            "epoch:  9\n",
            "Train Accuracy :  0.9944259259259259\n",
            "Validation Accuracy :  0.9768333333333333\n",
            "train loss 0.020057689377191035\n",
            "validation loss: 0.07752006684515841\n",
            "Test Accuracy 0.9757\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_layers=3\n",
        "activation=\"tanh\"\n",
        "Neural_Network ( num_layers, hidden_size, learning_rate, num_epochs, batch_size, beta, beta1,\n",
        "               beta2, epsilon, activation, optimizer,loss_function, weight_init,alpha ) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KP895pubt3LR",
        "outputId": "51f1b652-3fb4-4053-f6e0-7c343c96fc32"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nesterov Adam Adaptive Moments optimizer\n",
            "epoch:  0\n",
            "Train Accuracy :  0.9495\n",
            "Validation Accuracy :  0.9476666666666667\n",
            "train loss 0.27643358298753157\n",
            "validation loss: 0.1828453466555038\n",
            "epoch:  1\n",
            "Train Accuracy :  0.9637037037037037\n",
            "Validation Accuracy :  0.9596666666666667\n",
            "train loss 0.15369644576695943\n",
            "validation loss: 0.1385953077792235\n",
            "epoch:  2\n",
            "Train Accuracy :  0.9715\n",
            "Validation Accuracy :  0.9656666666666667\n",
            "train loss 0.11736703830724116\n",
            "validation loss: 0.11463542394917323\n",
            "epoch:  3\n",
            "Train Accuracy :  0.9777222222222223\n",
            "Validation Accuracy :  0.9701666666666666\n",
            "train loss 0.09457159473789367\n",
            "validation loss: 0.09936984092720069\n",
            "epoch:  4\n",
            "Train Accuracy :  0.9822777777777778\n",
            "Validation Accuracy :  0.9718333333333333\n",
            "train loss 0.07790765428413188\n",
            "validation loss: 0.08895333576328969\n",
            "epoch:  5\n",
            "Train Accuracy :  0.9853703703703703\n",
            "Validation Accuracy :  0.975\n",
            "train loss 0.0647591287202277\n",
            "validation loss: 0.08182624773770399\n",
            "epoch:  6\n",
            "Train Accuracy :  0.9885\n",
            "Validation Accuracy :  0.9751666666666666\n",
            "train loss 0.05393619681599218\n",
            "validation loss: 0.07706939716468444\n",
            "epoch:  7\n",
            "Train Accuracy :  0.9903333333333333\n",
            "Validation Accuracy :  0.9761666666666666\n",
            "train loss 0.044851981420391235\n",
            "validation loss: 0.07384328651049413\n",
            "epoch:  8\n",
            "Train Accuracy :  0.9920925925925926\n",
            "Validation Accuracy :  0.9763333333333334\n",
            "train loss 0.037166180463861165\n",
            "validation loss: 0.07160769228047278\n",
            "epoch:  9\n",
            "Train Accuracy :  0.9938888888888889\n",
            "Validation Accuracy :  0.9771666666666666\n",
            "train loss 0.03060007418955087\n",
            "validation loss: 0.07043883150330943\n",
            "Test Accuracy 0.9756\n"
          ]
        }
      ]
    }
  ]
}